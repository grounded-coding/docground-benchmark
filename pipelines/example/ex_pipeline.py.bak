# This file is placed in "pipelines/" and run via python -m pipelines.ex_pipeline

from src.data_collector import DSTCDataCollector, DummyDataCollector, BEGINDataCollector, DialDocDataCollector, TopicalChatDataCollector
from src.pipeline_evaluator import PipelineEvaluator
from src.eval_framework import DummyEval, LLEval, KnowledgeF1, BLEU, UniEval, KnowledgeBLEU, GEval
from src.eval_collector import DummyEvalCollector, DSTCHumanEvalCollector, BEGINHumanEvalCollector, DialDocEvalCollector, TopicalChatEvalCollector
import json
from utils.file_processing import load_data
from typing import List, Dict


type = 'spearman'
correlation_level = 'system'
unieval = UniEval()
unieval_dimensions = ["groundedness", "coherence"]
geval = GEval()
lleval = LLEval()
lleval_dimensions = ["accurate", "appropriate", "grounded"]
lleval_samp = LLEval(gen_config="configs/llama2/gen_config.json", name="LLEval")
bleu = KnowledgeBLEU()
bleu_dimensions = ["knowledge-bleu-4"]
kf1 = KnowledgeF1()
kf1_dimensions = ["knowledge-f1"]
refbleu = BLEU()
refbleu_dimensions = ["bleu-4"]

model_candidates = ["Original Ground Truth", "Argmax Decoding", "Nucleus Decoding (p = 0.5)", "Nucleus Decoding (p = 0.7)", "Nucleus Decoding (p = 0.3)", "New Human Generated"]

topicalchat_ue_collector = TopicalChatDataCollector("../topicalchat")
response_indices = topicalchat_ue_collector.get_samples_with_target(n=-1)
model_responses = topicalchat_ue_collector.get_pred_responses(response_indices, model_candidates)
tc_ue_eval_collector = TopicalChatEvalCollector("../topicalchat/restructured.json")

## TopicalChat USR
for framework, framework_dimensions in [(lleval, lleval_dimensions), (unieval, unieval_dimensions), (bleu, bleu_dimensions), (kf1, kf1_dimensions), (refbleu, refbleu_dimensions)]:
    framework_to_human_dimension_map = {framework_dimensions[0]: "groundedness"}
    if len(framework_dimensions) > 1:
        framework_to_human_dimension_map[framework_dimensions[1]] = "coherence"
    if len(framework_dimensions) > 2:
        framework_to_human_dimension_map[framework_dimensions[2]] = "groundedness"

    pipeline_evaluator = PipelineEvaluator(framework, tc_ue_eval_collector, topicalchat_ue_collector, framework_dimensions, framework_to_human_dimension_map, type, correlation_level, model_candidates)
    human_framework_correlations = pipeline_evaluator.run_pipeline(model_responses, response_indices)